{
    "collab_server" : "",
    "contents" : "\n\ntrain = read.csv('/home/lampros/kaggle_competitions/quora/train.csv', stringsAsFactors = F)\ndim(train); head(train); str(train)\n\n\n# single vector for the two questions\n\nvec = paste(train$question1, train$question2, sep = ' ')\n\nhead(vec); length(vec)\n\n\n# sklearn-modules for \"tfidf-vectorizer\" and \"truncated-svd\"\n\nreticulate::py_module_available('sklearn')\n\ntfid = reticulate::import('sklearn.feature_extraction.text')\n\ntfvec = tfid$TfidfVectorizer(min_df=3L, max_features=NULL, lowercase=T, \n                             \n                             strip_accents='unicode', analyzer='word',token_pattern='\\\\w{1,}',\n                             \n                             ngram_range=c(1L, 3L), use_idf=1L,smooth_idf=1L,sublinear_tf=1L,\n                             \n                             stop_words = 'english')\n\n\ntfvec$fit(vec)\n\nft = tfvec$transform(vec)\n\n# ft = tfvec$fit_transform(vec)\n# str(ft)\n# dat = ft$data\n\n\n\nSVD = reticulate::import('sklearn.decomposition')\n# res_svd = SVD$TruncatedSVD(n_components = 250L, n_iter = 5L, random_state = 1L)\n# res_svd = SVD$TruncatedSVD(n_components = 350L, n_iter = 5L, random_state = 1L)\n# res_svd = SVD$TruncatedSVD(n_components = 400L, n_iter = 5L, random_state = 1L)\nres_svd = SVD$TruncatedSVD(n_components = 500L, n_iter = 5L, random_state = 1L)\n\nres_svd$fit(ft)\n\nout_svd = res_svd$transform(ft)\n\n#out_svd = res_svd$fit_transform(ft)\nstr(out_svd); class(out_svd)\n# head(out_svd)\n\n\nNMF = SVD$NMF(n_components = 10L)\n\nNMF$fit(ft)\n\nout_nmf = NMF$transform(ft)\nstr(out_nmf)\n\n\n#-----------------------------------------------------------------\n\n# add pre-computed features\n\nPATH_x = '/home/lampros/kaggle_competitions/quora/save_features/basic_fuzzy_embed_X.csv'\n\nx = read.csv(PATH_x, header = T, sep = ',')\n\nxx = as.matrix(x)\n\ndim(xx)\n\n\n#-----------------------------------------------------------------\n\n# cbind features\n\n#ALL = cbind(xx, out_svd)\nALL = cbind(xx, out_svd, out_nmf)\ndim(ALL)\n\n#-----------------------------------------------------------------\n\nSCAL = reticulate::import('sklearn.preprocessing')\n\nsc = SCAL$StandardScaler()\n\nsvd_scaled = sc$fit_transform(ALL)\nstr(svd_scaled)\n\n# BUILDIN <- reticulate::import_builtins()\n\nNUMPY = reticulate::import(\"numpy\")\n\nsvd_scal_float = NUMPY$float32(svd_scaled)\nstr(svd_scal_float)\n\n\ny <- array(train$is_duplicate, dim = length(train$is_duplicate))\nstr(y)\n\n\nlibrary(kerasR)\n\nmod <- Sequential()\nmod$add(Dense(units = 250, activation = 'relu', input_shape = 307))\nmod$add(Dropout(0.25))\nmod$add(Dense(units = 250, activation = 'relu'))\nmod$add(Dropout(0.25))\nmod$add(Dense(units = 250, activation = 'relu'))\nmod$add(Dropout(0.25))\nmod$add(Dense(units = 250, activation = 'relu'))\nmod$add(Dropout(0.25))\nmod$add(Dense(1))\nmod$add(Activation(\"sigmoid\"))\n\n# keras_compile(mod,  loss = 'binary_crossentropy', optimizer = RMSprop(lr = 0.00025), metrics = 'accuracy')      # 343646/343646 [==============================] - 16s - loss: 0.4488 - acc: 0.7924 - val_loss: 0.4234 - val_acc: 0.8002\nkeras_compile(mod,  loss = 'binary_crossentropy', optimizer = 'adam', metrics = 'accuracy')                       # 343646/343646 [==============================] - 17s - loss: 0.3991 - acc: 0.8035 - val_loss: 0.3995 - val_acc: 0.8065\nkeras_fit(mod, svd_scal_float, y, batch_size = 32, epochs = 5, verbose = 1, validation_split = 0.15)              \n\n#========================================================================================================\n\nmod <- Sequential()\nmod$add(Dense(units = 350, input_shape = 307))\nmod$add(Dropout(rate = 0.25))\nmod$add(Activation(\"relu\"))\nmod$add(BatchNormalization())\nmod$add(Dense(units = 350))\nmod$add(Dropout(rate = 0.25))\nmod$add(Activation(\"relu\"))\nmod$add(BatchNormalization())\nmod$add(Dense(units = 350))\nmod$add(Dropout(rate = 0.25))\nmod$add(Activation(\"relu\"))\nmod$add(BatchNormalization())\nmod$add(Dense(units = 350))\nmod$add(Dropout(rate = 0.25))\nmod$add(Activation(\"relu\"))\nmod$add(BatchNormalization())\nmod$add(Dense(1))\n#mod$add(ActivityRegularization(l1 = 1))\nmod$add(Activation(\"sigmoid\"))\n\nkeras_compile(mod,  loss = 'binary_crossentropy', optimizer = 'adam', metrics = 'accuracy')\nkeras_fit(mod, svd_scal_float, y, batch_size = 32, epochs = 10, verbose = 1, validation_split = 0.15)        # 343646/343646 [==============================] - 37s - loss: 0.3835 - acc: 0.8138 - val_loss: 0.3817 - val_acc: 0.8118\n\n\n#========================================================================================================\n\nmod <- Sequential()\nmod$add(Dense(units = 350, input_shape = 307))\nmod$add(Dropout(rate = 0.25))\nmod$add(LeakyReLU())\nmod$add(BatchNormalization())\nmod$add(Dense(units = 350))\nmod$add(Dropout(rate = 0.25))\nmod$add(LeakyReLU())\nmod$add(BatchNormalization())\nmod$add(Dense(units = 350))\nmod$add(Dropout(rate = 0.25))\nmod$add(LeakyReLU())\nmod$add(BatchNormalization())\nmod$add(Dense(units = 350))\nmod$add(Dropout(rate = 0.25))\nmod$add(LeakyReLU())\nmod$add(BatchNormalization())\nmod$add(Dense(1))\nmod$add(Activation(\"sigmoid\"))\n\nkeras_compile(mod,  loss = 'binary_crossentropy', optimizer = 'adam', metrics = 'accuracy')\nkeras_fit(mod, svd_scal_float, y, batch_size = 32, epochs = 10, verbose = 1, validation_split = 0.15)   # 343646/343646 [==============================] - 37s - loss: 0.4061 - acc: 0.7980 - val_loss: 0.3921 - val_acc: 0.8054\n\n#==========================================================================================================\n\nmod <- Sequential()\nmod$add(Dense(units = 350, input_shape = 307))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(1))\nmod$add(Activation('sigmoid'))\n\nkeras_compile(mod,  loss = 'binary_crossentropy', optimizer = 'adam', metrics = 'accuracy')\nkeras_fit(mod, svd_scal_float, y, batch_size = 32, epochs = 10, verbose = 1, validation_split = 0.15)   # 343646/343646 [==============================] - 57s - loss: 0.3696 - acc: 0.8213 - val_loss: 0.3713 - val_acc: 0.8177\n                                                                                                        \n#==========================================================================================================\n\nmod <- Sequential()\nmod$add(Dense(units = 350, input_shape = 307))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(1))\nmod$add(Activation('sigmoid'))\n\nkeras_compile(mod,  loss = 'binary_crossentropy', optimizer = 'adam', metrics = 'accuracy')\nkeras_fit(mod, svd_scal_float, y, batch_size = 32, epochs = 20, verbose = 1, validation_split = 0.15)   # 343646/343646 [==============================] - 64s - loss: 0.3509 - acc: 0.8324 - val_loss: 0.3665 - val_acc: 0.8241\n\n#==========================================================================================================\n\n# compute weights for unbalanced data [ https://groups.google.com/forum/#!topic/keras-users/MUO6v3kRHUw ]\n\nUTILS = reticulate::import('sklearn.utils')\nw = UTILS$compute_class_weight('balanced', c(0, 1), y)\nw\n\n\n#==========================================================================================================\n\nmod <- Sequential()\nmod$add(Dense(units = 350, input_shape = 307))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 350))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(1))\nmod$add(Activation('sigmoid'))\n\nkeras_compile(mod,  loss = 'binary_crossentropy', optimizer = 'adam', metrics = 'accuracy')\n\nkeras_fit(mod, svd_scal_float, y, batch_size = 64, epochs = 25, verbose = 1,                    # try also shuffling the data using \"shuffle = TRUE\"\n                                                                                                # using  \"class_weight = w\" does not help [ SEE previous UTILS function ]\n          validation_split = 0.15)                                                  # 343646/343646 [==============================] - 40s - loss: 0.3360 - acc: 0.8414 - val_loss: 0.3577 - val_acc: 0.8270\n\n\n#==========================================================================================================\n\n\nmod <- Sequential()\nmod$add(Dense(units = 450, input_shape = 407))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 450))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 450))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 450))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 450))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 450))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 450))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(1))\nmod$add(Activation('sigmoid'))\n\nkeras_compile(mod,  loss = 'binary_crossentropy', optimizer = 'adam', metrics = 'accuracy')\n\nkeras_fit(mod, svd_scal_float, y, batch_size = 64, epochs = 30, verbose = 1,                    # try also shuffling the data using \"shuffle = TRUE\"\n          # using  \"class_weight = w\" does not help [ SEE previous UTILS function ]\n          validation_split = 0.15)                                                  # TEST : val_loss: 0.3544 - val_acc: 0.83\n\n#==========================================================================================================\n\n\nmod <- Sequential()\nmod$add(Dense(units = 500, input_shape = 457))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 500))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 500))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 500))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 500))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 500))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 500))\nmod$add(PReLU())\nmod$add(Dropout(0.25))\nmod$add(BatchNormalization())\n\nmod$add(Dense(1))\nmod$add(Activation('sigmoid'))\n\nkeras_compile(mod,  loss = 'binary_crossentropy', optimizer = 'adam', metrics = 'accuracy')\n\nkeras_fit(mod, svd_scal_float, y, batch_size = 64, epochs = 30, verbose = 1,                    # try also shuffling the data using \"shuffle = TRUE\"\n          # using  \"class_weight = w\" does not help [ SEE previous UTILS function ]\n          validation_split = 0.15)                                                  # 343646/343646 [==============================] - 53s - loss: 0.2780 - acc: 0.8738 - val_loss: 0.3507 - val_acc: 0.8372\n\n#==========================================================================================================\n\n\nmod <- Sequential()\nmod$add(Dense(units = 600, input_shape = 557))\nmod$add(PReLU())\nmod$add(Dropout(0.35))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 600))\nmod$add(PReLU())\nmod$add(Dropout(0.35))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 600))\nmod$add(PReLU())\nmod$add(Dropout(0.35))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 600))\nmod$add(PReLU())\nmod$add(Dropout(0.35))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 600))\nmod$add(PReLU())\nmod$add(Dropout(0.35))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 600))\nmod$add(PReLU())\nmod$add(Dropout(0.35))\nmod$add(BatchNormalization())\n\nmod$add(Dense(units = 600))\nmod$add(PReLU())\nmod$add(Dropout(0.35))\nmod$add(BatchNormalization())\n\nmod$add(Dense(1))\nmod$add(Activation('sigmoid'))\n\nkeras_compile(mod,  loss = 'binary_crossentropy', optimizer = 'adam', metrics = 'accuracy')\n\nkeras_fit(mod, svd_scal_float, y, batch_size = 64, epochs = 30, verbose = 1,                    # shuffling the data defaults to \"shuffle = TRUE\"\n          # using  \"class_weight = w\" does not help [ SEE previous UTILS function ]\n          validation_split = 0.15)                                                  # 343646/343646 [==============================] - 54s - loss: 0.2786 - acc: 0.8731 - val_loss: 0.3449 - val_acc: 0.8393\n\n",
    "created" : 1492096236242.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2604896826",
    "id" : "C508174B",
    "lastKnownWriteTime" : 1492018032,
    "last_content_update" : 1492018032,
    "path" : "/media/lampros/C2A8-A74D/truncated_svd_tfidf.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}